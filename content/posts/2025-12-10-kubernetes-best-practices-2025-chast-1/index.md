---
title: "Лучшие практики конфигурирования Kubernetes в 2025 — Часть 1: Фундамент и развертывание без боли (1–15)"
date: 2025-12-10T13:41:00+03:00
description: "Серия заметок: лучшие практики Kubernetes в формате живой статьи. Эта часть — одна из глав; все примеры и команды сохранены."
thumbnail: "images/image.png"
tags: [sre, kubernetes]
---

Kubernetes — это не «оркестратор», это строгий преподаватель: он помнит всё, не прощает невнятные договорённости и всегда проверяет домашку в самый неудобный момент (обычно во время обновления кластера).

Часть 1 — про фундамент: версии API, Git как источник истины, дисциплина YAML и базовый набор привычек для развертывания workloads без “ну оно же вчера работало”. Это те вещи, которые дают самый быстрый эффект и чаще всего окупаются первым же стабильным релизом.

**Серия:** Часть 1 из 6. (Ссылки на остальные части можно проставить после объединения.)

### В этой части
- 1. Всегда используйте актуальную стабильную версию API
- 2. Храните всю конфигурацию в системе контроля версий
- 3. Соблюдайте принцип DRY с Kustomize или Helm
- 4. Используйте Namespaces для логического разделения
- 5. Пишите конфигурацию в YAML, а не JSON
- 6. Правильно обрабатывайте булевы значения в YAML
- 7. Держите манифесты минимальными
- 8. Группируйте связанные объекты в одном файле
- 9. Никогда не используйте голые Pod'ы в продакшене
- 10. Используйте Deployments для stateless приложений
- 11. Используйте Jobs для одноразовых задач
- 12. Всегда настраивайте проверки работоспособности
- 13. Правильно настраивайте Rolling Updates
- 14. Используйте Pod Disruption Budgets (PDB)
- 15. Реализуйте Pod Anti-Affinity для высокой доступности

### 1. Всегда используйте актуальную стабильную версию API

**Почему важно:** Чтобы апдейты кластера не превращались в лотерею: deprecated API ломаются внезапно и массово.

**Если не делать:** На обновлении Kubernetes получите ошибки apply/rollout и простой, потому что ресурсы перестанут приниматься API-server’ом.

Kubernetes API быстро развиваются. Использование устаревших версий API приводит к сломанным деплойментам при обновлении кластера.

**Всегда проверяйте доступные версии API:**

```bash
kubectl api-resources
kubectl api-versions
```

**Совет:** Используйте инструменты вроде `kubent` (Kube No Trouble) для обнаружения устаревших API в ваших манифестах перед обновлением:

```bash
kubent --helm3 --exit-error
```
### 2. Храните всю конфигурацию в системе контроля версий

**Почему важно:** Git — это аудит, откат и единый источник истины. Без него конфигурация быстро превращается в «у кого на ноутбуке правильнее».

**Если не делать:** Конфиг-дрифт, невозможность воспроизвести состояние, сложный разбор инцидентов и «ручные правки на проде» без следов.

**Никогда не применяйте манифесты напрямую с локальной машины.** Каждый конфигурационный файл Kubernetes должен находиться в Git (или другой системе контроля версий).

**Преимущества версионируемой конфигурации:**

*   Мгновенный откат при неудачных деплоях
*   Полный аудит-лог того, кто что и когда изменил
*   Простая совместная работа и код-ревью
*   Воспроизводимые настройки кластера в разных окружениях

**Рекомендуемая структура папок:**

```
kubernetes/
├── base/
│   ├── deployment.yaml
│   ├── service.yaml
│   └── configmap.yaml
├── overlays/
│   ├── development/
│   ├── staging/
│   └── production/
└── README.md
```
### 3. Соблюдайте принцип DRY с Kustomize или Helm

**Почему важно:** DRY уменьшает вероятность человеческих ошибок и расхождений между окружениями.

**Если не делать:** Dev/stage/prod начнут вести себя по‑разному; правки будут забываться в одном из overlays/values и выстрелят позже.

Не повторяйтесь в разных окружениях. Используйте **Kustomize** (встроен в kubectl) или **Helm** для управления вариациями конфигурации для разных окружений.

**Пример Kustomize:**

```yaml
# kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - deployment.yaml
  - service.yaml
patches:
  - path: replica-patch.yaml
```
### 4. Используйте Namespaces для логического разделения

**Почему важно:** Namespaces дают изоляцию по ресурсам, доступам и политиками — это базовая гигиена multi-team кластера.

**Если не делать:** Ресурсы и права смешаются; квоты/политики будет сложно применить; случайные удаления/селекторы станут опаснее.

Namespaces предоставляют логические границы внутри кластера. Используйте их для:

*   Разделения окружений (dev, staging, prod)
*   Изоляции команд или проектов
*   Применения квот ресурсов и сетевых политик

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    environment: production
    team: platform
```

Стандарты конфигурирования YAML
---------------------------------
### 5. Пишите конфигурацию в YAML, а не JSON

**Почему важно:** YAML читается и ревьюится людьми: комментарии, компактность, принятый стандарт экосистемы.

**Если не делать:** Поддержка и ревью усложнятся; возрастёт шанс «незаметных» ошибок и копипасты без понимания.

Хотя Kubernetes принимает и YAML, и JSON, **YAML является стандартом сообщества**. Он более читаем, поддерживает комментарии и проще в поддержке.

**Лучшие практики YAML:**

```yaml
# Хорошо: Чистый, читаемый YAML
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-application
  labels:
    app.kubernetes.io/name: web-app
    app.kubernetes.io/component: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/name: web-app
```
### 6. Правильно обрабатывайте булевы значения в YAML

**Почему важно:** Неоднозначные булевы значения — классический источник скрытых багов парсинга.

**Если не делать:** Получите конфиг, который в одном месте трактуется как bool, в другом как string/undefined — и поведение станет непредсказуемым.

YAML имеет хитрый парсинг булевых значений. Разные версии YAML интерпретируют значения по-разному.

**Всегда используйте явные булевы значения:**

```
# ✅ ПРАВИЛЬНО - Всегда используйте true/false
enabled: true
secure: false

# ❌ ИЗБЕГАЙТЕ - Это может вызвать проблемы
enabled: yes    # Может не распарситься корректно
enabled: on     # Неоднозначно
enabled: "yes"  # Это строка, а не булево значение
```
### 7. Держите манифесты минимальными

**Почему важно:** Минимальные манифесты проще сопровождать и безопаснее менять: вы не фиксируете лишние дефолты.

**Если не делать:** Будете тащить шум в diff’ах, сложнее апгрейдиться и выше риск конфликтов при server-side apply/helm.

Не устанавливайте значения, которые Kubernetes обрабатывает по умолчанию. Минимальные манифесты:

*   Проще читать и ревьюить
*   Менее подвержены ошибкам
*   Проще поддерживать

**Пример минимального Deployment:**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.25
        ports:
        - containerPort: 80
```
### 8. Группируйте связанные объекты в одном файле

**Почему важно:** Связанные объекты удобнее деплоить атомарно и ревьюить как единый change-set.

**Если не делать:** Вы начнёте применять ресурсы «в разнобой», ловить гонки (например, Deployment без Service/ConfigMap) и неочевидные зависимости.

Если ваши Deployment, Service и ConfigMap относятся к одному приложению, поместите их в один файл, разделив через `---`. 

Тут важно понимать, что речь про организационную атомарность (в одном файле/изменении), а не как гарантия Kubernetes “все ресурсы применились или ни один”.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  # ... спецификация deployment
---
apiVersion: v1
kind: Service
metadata:
  name: myapp
spec:
  # ... спецификация service
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: myapp-config
data:
  # ... данные конфигурации
```


**Преимущества:**

*   Проще управлять связанными ресурсами
*   Лучшая организация в системе контроля версий

Лучшие практики конфигурирования Pod и Workloads
-------------------------------------------------
### 9. Никогда не используйте голые Pod'ы в продакшене

**Почему важно:** Голый Pod не имеет self-healing/rolling update/масштабирования — это одноразовая граната.

**Если не делать:** При падении/эвикте Pod не восстановится как надо; деплой станет ручным; прод быстро превратится в «зоопарк» исключений.

**Голые Pod'ы** (Pod'ы без контроллера) опасны в продакшене, потому что:

*   Они не перепланируются при отказе ноды
*   Они не масштабируются автоматически
*   У них нет возможности rolling update

**Всегда используйте контроллеры:**

|Сценарий использования     |Контроллер  |
|---------------------------|------------|
|Долгоживущие приложения    |Deployment  |
|Stateful приложения        |StatefulSet |
|Демоны на уровне ноды      |DaemonSet   |
|Пакетная обработка         |Job         |
|Запланированные задачи     |CronJob     |

### 10. Используйте Deployments для Stateless приложений

**Почему важно:** Deployment — стандартный контроллер для stateless: rollout, стратегии, откат, реплики.

**Если не делать:** Обновления станут опасными; откаты — ручными; стабильность при релизах упадёт.

Deployments — это стандарт для запуска Stateless приложений:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
  annotations:
    kubernetes.io/description: "REST API сервер для данных клиентов"
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: api-server
  template:
    metadata:
      labels:
        app: api-server
        version: v2.1.0
    spec:
      containers:
      - name: api
        image: myregistry/api-server:v2.1.0
        ports:
        - containerPort: 8080
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
```
### 11. Используйте Jobs для одноразовых задач

**Почему важно:** Job/CronJob дают правильную семантику завершения и ретраев для batch задач.

**Если не делать:** Одноразовые задачи начнут жить как сервисы, зависать, перезапускаться не так и портить данные/миграции.

Jobs идеальны для:

*   Пакетной обработки
*   Импорта/экспорта данных

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: db-migration
spec:
  backoffLimit: 3
  ttlSecondsAfterFinished: 3600
  template:
    spec:
      restartPolicy: OnFailure
      containers:
      - name: migration
        image: myregistry/db-migrate:v1.0
        command: ["./migrate", "--target=latest"]
```
### 12. Всегда настраивайте пробы

**Почему важно:** Probes — это контракт готовности и живости. Без них балансировщик и kubelet не знают, что есть проблема.

**Если не делать:** Ловите трафик в неготовые Pod’ы, вечные рестарты, долгие деплои и инциденты «у нас всё поднялось, но не работает».

Проверки работоспособности критичны для надёжности в продакшене:

**Liveness Probe:** Перезапускает зависшие контейнеры 
**Readiness Probe:** Удаляет pod'ы из service endpoints до готовности 
**Startup Probe:** Обрабатывает медленно стартующие контейнеры

```yaml
containers:
- name: app
  image: myapp:v1
  livenessProbe:
    httpGet:
      path: /healthz
      port: 8080
    initialDelaySeconds: 30
    periodSeconds: 10
    failureThreshold: 3
  readinessProbe:
    httpGet:
      path: /ready
      port: 8080
    initialDelaySeconds: 5
    periodSeconds: 5
    successThreshold: 1
  startupProbe:
    httpGet:
      path: /healthz
      port: 8080
    failureThreshold: 30
    periodSeconds: 10
```

Стратегии конфигурирования Deployment
--------------------------------------
### 13. Правильно настраивайте Rolling Updates

**Почему важно:** RollingUpdate параметры определяют простои и скорость релиза.

**Если не делать:** Слишком агрессивные значения дадут деградацию/простой; слишком осторожные — затянут релиз и усложнят откат.

Rolling updates минимизируют простой, постепенно заменяя старые pod'ы новыми:

```yaml
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 25%        # Макс. дополнительных pod'ов во время обновления
      maxUnavailable: 0    # Требование нулевого простоя
```


**Объяснение ключевых настроек:**

*   `maxSurge`: Сколько дополнительных pod'ов может существовать во время развертывания
*   `maxUnavailable`: Сколько pod'ов может быть недоступно во время развертывания

### 14. Используйте Pod Disruption Budgets (PDB)

**Почему важно:** PDB защищает от одновременного выноса всех реплик при дренажах/апгрейдах.

**Если не делать:** Во время maintenance можно случайно «выключить» сервис целиком, даже если реплик много.

PDB защищают ваше приложение во время плановых нарушений (дренаж нод, обновления кластера):

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: api-pdb
spec:
  minAvailable: 2    # Или используйте maxUnavailable
  selector:
    matchLabels:
      app: api-server
```
### 15. Реализуйте Pod Anti-Affinity для высокой доступности

**Почему важно:** Anti-affinity/распределение — это страховка от отказа одной ноды/зоны.

**Если не делать:** Одна нода/зона упала — и вместе с ней упали все реплики, потому что они жили рядом.

Распределяйте pod'ы по нодам и зонам для выживания при отказах:

```yaml
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchLabels:
              app: api-server
          topologyKey: kubernetes.io/hostname
      - weight: 50
        podAffinityTerm:
          labelSelector:
            matchLabels:
              app: api-server
          topologyKey: topology.kubernetes.io/zone
```
